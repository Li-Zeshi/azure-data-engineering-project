如何在azure 搭建数据管道，批量复制azure sql database的数据并进行清洗和转化，这里是一份保姆级教程：

一、data ingestion
目标：将 SQL Server 多张表动态复制到 Azure Data Lake Storage Gen2 的 Bronze 层
整体架构概览 [SQL Server] → [ADF Pipeline] → [ForEach 循环] → [Copy Activity] → [Parquet Dataset] → [ADLS Gen2 /bronze/SchemaName/TableName/]

新建一个azure sql database 选择示例数据，azure会自动导入我们需要使用的源数据AdventureWorks数据库
AdventureWorks数据库是微软为SQL Server提供的流行且广泛使用的示例数据库。它是学习和实践SQL查询、数据库设计和各种数据相关任务的绝佳资源。
现在我需要把在 SQL Server 下Azure SQL Database 中存入的AdventureWorks里的表批量全部通过adf复制过来
在我的 Udemy 项目中，这个SQL就是用在pipeline的Lookup Activity 里，来获取 SalesLT 下所有表名，然后用 ForEach 活动对每个表执行一个 Copy 操作，实现动态数据管道。

Lookup 设置新建好含有AdventureWorks数据库的数据集以后，点击查询，在查询框里写如下语句：
SELECT s.name AS SchemaName, t.name AS TableName  
# Selects the name of the schema and the name of the table, and assigns them aliases SchemaName and TableName.
FROM sys.tables t  
# sys.tables is a system catalog view that contains a row for each user-defined table in the current database. Alias t is used for brevity.
INNER JOIN sys.schemas s ON t.schema_id = s.schema_id  
# Performs an inner join between sys.tables and sys.schemas using the schema_id field, so we can find out which schema each table belongs to.
WHERE s.name = 'SalesLT'; 
# Filters the results to only include tables that belong to the schema named 'SalesLT'


你可能会问：
    # 1 sys 是什么？
    sys 是 SQL Server 中的“系统架构（schema）”，也就是系统对象所在的专属 schema。它包含了数据库引擎维护的系统视图（System Views）、函数、存储过程等。
    就像你写表用 SalesLT.Customer 来指定 schema 为 SalesLT，
    sys.tables 就是指 sys 这个系统 schema 下面的 tables 系统视图。
    把数据库看成一个图书馆：普通用户建的表（如 SalesLT.Customer）是普通图书。sys.tables、sys.columns、sys.schemas 等是图书馆的“目录索引”
    sys 是专门存这些“目录、索引、管理信息”的专属区域
    Schema 是数据库中用来组织对象（表、视图、存储过程等）的一种方式。类似于文件夹，把不同表分类归组。一个用户可以有多个 schema，比如 SalesLT, HR, dbo, Production 等。

    # 2 为什么 ADF 和数据工程常用 sys？
    因为数据工程师要做“自动化 + 动态数据管道”，必须先知道：
    哪些表存在？
    哪些列是啥？
    属于哪个 schema？
    这些信息都在 sys 下的系统视图中，尤其是：sys.tables  sys.columns  sys.schemas
    
    # 3 sys.tables  和 sys.schemas 里有什么？ 
    sys.tables 和 sys.schemas 是两个 SQL Server 系统视图。用于列出数据库中所有用户定义的表（user tables）。

 sys.tables包含的典型字段有：
| 列名           | 含义                  |
| --------------| ------------------- |
| object_id     | 表的唯一 ID（可用于查找列、约束等）｜ 
| name          | 表的名称                |
| schema_id     | 表所属的 schema ID      |
| create_date   | 创建时间                |
| modify_date   | 最后修改时间              |
| is_ms_shipped | 是否系统内部表（我们通常只看 `0`） |

     # sys.schemas 是什么？
sys.schemas 是一个系统视图，记录数据库中所有的 schema（模式）的定义。
sys.schemas 中包含字段：
| 列名          | 含义                           |
| -------------| ------------------------------|
| schema_id    | Schema 的唯一 ID（用来连接 sys.tables）|
| name         | Schema 的名字，例如 SalesLT, dbo  |
| principal_id | 创建这个 schema 的用户 ID（通常用不着）  |

每张表（sys.tables）都属于某个 schema（sys.schemas）。它们之间通过 schema_id 连接：
FROM sys.tables t
JOIN sys.schemas s ON t.schema_id = s.schema_id
这样可以查出：
每张表的名字（来自 sys.tables.name）
每张表所属的 schema（来自 sys.schemas.name）

可视化理解：
┌───────────────┐        ┌────────────────┐
│   sys.tables  │        │   sys.schemas  │
├───────────────┤        ├────────────────┤
│ name          │        │ name           │
│ schema_id  ───────────▶│ schema_id      │
└───────────────┘        └────────────────┘

最终结果（查询输出长这样）：
| SchemaName | TableName |
| ---------- | --------- |
| SalesLT    | Address   |
| SalesLT    | Customer  |
| SalesLT    | Product   |
| ...        | ...       |

  # 为什么数据工程中常用它们？
    在数据管道中，你经常需要动态获取：某个 schema 下有哪些表？自动循环处理这些表的数据。
    用 sys.tables 和 sys.schemas 查询就能实现“从数据库自身结构中获取信息”的能力，这就是所谓的 "元数据驱动的管道（Metadata-driven pipeline）"
    这个在 ADF 中非常常见，特别是在动态构建 Copy Activity 或 Lookup Activity 的数据源时。

接下来在adf中新建数据管道
然后添加look up活动
设置选择源数据集，连接sql server，选择数据库，不用选择任何表。
设置-使用查询 写sql查询语句-然后点上面的preview data
然后我们点击debug运行这个look up activity。运行成功我们检查output 里面是json格式储存的我们用sql查询出的表格信息。表的信息都存在output里的'value'这个key对应的值（是个列表）里
然后搜ForEach activity拖到iook up旁边。 点ForEach activity点设置，底下有个item点添加动态内容 [Alt+Shift+P]
点击Lookup for all tables 活动输出自动生成表达式，手动结尾.value

    # 为什么look up要连接foreach？ForEach 活动的本质是什么？
    在 Azure Data Factory (ADF) 中，ForEach 活动的本质是让你对一组元素（数组）进行迭代，逐个执行相同的子操作（例如 Copy 活动），实现批量自动处理的目的。
    可以把它理解成 Azure Data Factory 里的 “循环语句”，就像 Python 中的：for item in list: 
    ForEach 活动 里面加入的活动就相当于Python 中的：for item in list:  之后对item进行的操作 比如print（item）等
    ForEach 的核心本质：
    输入是一个数组，你需要传给 ForEach 一个数组作为迭代对象，数组可以来源于：手动写死的值（例如：[1,2,3]） / 从 Lookup 活动中查询数据库表、文件等 / 管道参数传入
    每一次循环都执行一组子活动，你可以在 ForEach 中放入一个或多个活动，例如：Copy Data、Execute Pipeline、Set Variable、If Condition、Web Activity 等
    这些活动会对 item()（即当前这次循环的项）进行处理。
    # 支持并发执行
    你可以设置 ForEach 的 “并发度（Batch Count）”，比如：
    并发度为 1：顺序执行每个项；
    并发度为 5：最多同时处理 5 个项，加快速度。
    

清楚了ForEach 活动的本质，接下来我们继续搭建自动化复制表格进目标数据湖的数据管道
在ForEach 里点活动，点编辑，在内部添加copy activity。选择数据集，在下面写查询语句@{concat('select * from',item().SchemaName,'.',item().TableName)}
    这个查询语句是什么意思呢？
    这是一段 ADF 的表达式语言（即 dynamic content expressions），用于动态构造 SQL 查询语句。查询语句的目的是不断迭代直到select完所有的表
    1 @{...}：这是 ADF 的表达式语法，表示其中的内容是 动态计算的，而不是静态文本。
    2 concat(...)：是 ADF 的字符串拼接函数。
    3 'select * from '：这是拼接的第一个字符串，表示 SQL 查询的开头。
    4 item().SchemaName：在 ForEach 循环中，item() 表示当前循环到的那个元素（可能是一个 JSON 对象），.SchemaName 是它的一个字段（值可能是 dbo 或 sales 等）。
    5 '.'：是连接 schema 和表名的点号（SQL 中完整表名格式是 schema.table）。
    6 item().TableName：当前项中的表名字段。
    假设你的 ForEach 正在循环一个这样的数组（从某个 Lookup 活动、Dataset 或参数中传入）：
    [
      { "SchemaName": "sales", "TableName": "Customers" },
      { "SchemaName": "hr", "TableName": "Employees" }
    ]
    那么：
    第一次循环：生成的 SQL 会是select * from sales.Customers
    第二次循环：生成的 SQL 会是select * from hr.Employees
    这一过程将会不断迭代直到select完所有的表
    使用场景：
    这种写法非常适用于 批量复制多个表的场景，比如：
    1 从不同的 schema/table 中提取数据；
    2 自动复制多个表而无需为每个表单独建一个管道；
    3 动态构建查询语句，提高管道的通用性和自动化程度。

接下来是配置同步sink，我们要把复制过来的数据存在数据湖里。选择sink，新建dataset，选择Azure Data Lake Storage Gen2，选择parquet格式文件。
  # 为什么选择 Parquet 格式作为 Sink 文件格式？
    这是个非常关键的问题，涉及到 数据湖建模标准、性能优化 和 数据工程实践中的文件格式选择策略。
    为什么选 Parquet 格式作为 Sink？
    1. 🧠 Parquet 是列式存储格式，适合分析型查询
    相比 CSV/JSON 这类 行式格式，Parquet 的 列式存储 能够只读取分析时真正需要的列，显著减少 I/O 和查询延迟。
    特别适合用于 Spark、Databricks、Synapse、Power BI 等下游数据分析工具。
    2. 🚀 高压缩率 + 高性能
    Parquet 格式默认支持 压缩（如 Snappy），体积更小，传输和存储效率更高。
    适合大数据量场景，尤其是在数据湖中存储“青铜层”原始数据时，用 Parquet 可以节省大量成本。
    3. 🔄 强类型 + Schema 支持
    Parquet 是支持 schema 的格式，ADF 能够明确字段类型（如整数、浮点、时间戳等），不会像 CSV 那样丢失类型信息。
    方便数据治理和后续的数据验证、转换。
    4. 🧱 符合数据湖层级建模标准（Bronze → Silver → Gold）
    在 Lakehouse 架构中，青铜层（Bronze Layer）用于保存原始数据副本；
    这些数据多数采用 Parquet，因为它：不易被修改（Immutable），易于后续转换为银层（清洗后结构化数据），更适合做批处理和 SQL 查询分析

接下来我们要设置文件存储路径最开头的文件系统的名字
选择我们之前创建的链接的服务 AzureDataLakeStorage1，选择之前创建的储存的文件路径bronze青铜层
我们点击sink下面指定的数据集parquetcopytable 进去之后设置两个参数，一个是SchemaName 一个是TableName 
返回到数据管道中我们就可以看到sink下面出现了两个参数，SchemaName 和 TableName 我们可以输入动态的值@item().SchemaName和@item().TableName
  # 为什么要设置 SchemaName 和 TableName 参数？
    在 ADF 的 Copy Data 活动中，你设置 Sink（目的地）为 Data Lake Storage Gen2 的 Parquet 文件格式，但你希望：每张表的数据被分别保存为一个 Parquet 文件，而不是都写入同一个文件。
    因此你需要让 ADF 知道：当前这张表叫什么（TableName）；它属于哪个 schema（SchemaName）。从而决定文件保存在哪里。
    为此需要设置 Sink 数据集中的两个参数：SchemaName 和 TableName，并在 ForEach 的每次循环中动态传入。
  # 为什么是 @item().SchemaName 和 @item().TableName？
    因为在 ForEach 中循环的是一个 数组对象，每一项都像这样：
    {
      "SchemaName": "sales",
      "TableName": "Customers"
    }
    而 ADF 中 item() 表示当前循环项。所以：
    @item().SchemaName 表示当前项的 schema 名称，如 sales
    @item().TableName 表示当前项的表名，如 Customers

接下来我们要设置文件存储路径中间的directory目录名，添加动态内容以生成目录结构  
    # 添加动态内容 @{concat(dataset().SchemaName,'/',dataset().TableName)}代码的含义：？
    这段表达式涉及 ADF 的 数据集参数化机制 和 文件存储路径的自动生成逻辑。
    在 ADF 中：
    dataset() 是一种表达式函数，用于访问当前数据集（Dataset）中的参数。
    你在创建 Parquet 类型的数据集时，为了让不同表的数据能保存到不同的文件夹，你设置了两个参数：SchemaName、TableName
    所以你可以通过：dataset().SchemaName来访问这些参数的值，并将它们用在路径配置中。
    为什么要设置目录路径？用于填充 是为了填Parquet 文件的 directory（目录）字段，把数据保存到如下结构的路径下：/bronze/sales/Customers/
    原因包括：
    | 好处        |               说明                                                                
    | ----------- | ----------------------------------------------------------------- |
    | 📂 分类清晰    | 不同 Schema/表名的表各存在不同文件夹，方便管理和下游读取                                  
    | 📈 支持增量写入 | 将不同批次的数据写入同一目录下，不断追加                                              
    | 🧹 清洗管理方便| 可以批量删除某个 Schema 下的全部文件或表数据                                        
    | 🔁 可迭代复用  | 下游系统如 Spark、Databricks 可以用 wildcard (`bronze/*/*.parquet`) 读入所有文件 

接下来我们设置文件储存路径最后的文件名
同样添加动态内容，代码：@{concat(dataset().TableName,'.Parquet')}
    # 这段表达式是用来动态生成每个导出文件的名字，确保你复制的每张表都能保存成独立的 .parquet 文件。
    这句代码的意思是：生成当前表名 + .parquet 后缀组成的文件名
    例如：
    当前表是 Customers → 生成文件名 Customers.Parquet
    当前表是 Invoices → 生成文件名 Invoices.Parquet

也就是说，通过设置文件存储路径最开头的文件系统的名字 / 设置文件存储路径中间的directory目录名 / 设置文件储存路径最后的文件名 三步，最终你将导出为：
bronze/sales/Customers/Customers.parquet
bronze/hr/Employees/Employees.parquet
……
……
……


流程图示
1 ForEach 输入数组：
[
  { "SchemaName": "sales", "TableName": "Customers" },
  { "SchemaName": "hr", "TableName": "Employees" }
]
2 ForEach 每次循环设置：
Source SQL: select * from @{item().SchemaName}.@{item().TableName}
Sink 参数：
SchemaName = @item().SchemaName
TableName = @item().TableName
3 Sink 数据集路径设置：/bronze/@{dataset().SchemaName}/@{dataset().TableName}.parquet

下一步我们点击Publish all发布全部，运行管道可以用debug 也可以add trigger
这里我们add trigger 添加触发器 点tigger now我们能触发一次
第一次运行失败：
问题在于在 Copy 活动的 Source Query 里写了这样一句表达式：@{concat('select * from', item().SchemaName, '.', item().TableName)}
注意： 'from' 和 item().SchemaName 之间没有空格，拼接后变成了：select * fromSalesLT.Customers 导致这就是语法错误！导致copy 失败
改掉就ok了


关于添加动态内容的原因的整理：
| 使用位置                   | 动态内容写法                                                                | 意义        | 原因     |
| ----------------------    | -----------------------------------------------------------------------  | ---------   | ------ |
| ForEach → Items           | `@activity('Lookup Tables').output.value`                                | 获取所有表数组   | 用于循环   | 
# .output.value 表示 Lookup 查询返回的所有结果记录（即多个表名、模式名组合）。ForEach 需要一个数组类型的输入，才能循环执行 Copy 活动。
| Copy Source → Query       | `@{concat('select * from ', item().SchemaName, '.', item().TableName)}`  | 动态构建 SQL  | 每张表都不同，拼接成每一张表的 SQL 查询语句 |
| Copy Sink → 参数传入        | `@item().SchemaName` `@item().TableName`                                | 将表信息传给数据集 | 用于路径生成 |
# ADF 不直接支持在 Sink 的路径中写 item().，所以你需要将它传给 Parquet 数据集的参数。
| Parquet Dataset → 目录路径  | `@{concat(dataset().SchemaName, '/', dataset().TableName)}`             | 建立目录结构    | 按表分类存储 ，dataset().SchemaName 是访问传进来的参数（不是 item(|
| Parquet Dataset → 文件名   | `@{concat(dataset().TableName, '.parquet')}`                             | 文件命名      | 每表一个文件 ，拼接表名与文件扩展名，确保每个表的数据导出为单独文件|




二、data transform
打开我们之前创建的databricks。
 Databricks 教学内容常见结构
Databricks 基础概念
    什么是 Databricks（Lakehouse 架构）
    它与 Azure、Spark、Delta Lake 的关系
    Workspace、Cluster、Notebook 的作用和使用方法
工作环境设置
    创建 Azure Databricks 服务
    配置 Cluster（通常用 Standard_DS3_v2、Runtime 版本）
    连接 Data Lake、SQL Database、ADF 等资源
使用 Notebook 编程
    使用 %python、%sql、%scala 魔法命令
    运行 PySpark 代码读取和处理数据
    数据清洗、转换、分析流程实操
读写数据（尤其是与 ADF 配合）
    读取 CSV、Parquet、Delta 格式
    写入到 Azure Data Lake 或 Delta Table
    使用 spark.read.format() 和 df.write.format() 等方法
Delta Lake 实战
    建立增量处理（SCD Type 1/2）
    使用 MERGE、VACUUM、OPTIMIZE
    管理版本历史和时间旅行（Time Travel）
与 ADF 配合自动化流程
    ADF 调用 Notebook
    参数传递、调试和监控
    构建端到端的数据管道

我们点击compute 点击create compute
先改一下cluster的名字
Azure Data Lake Storage 凭据传递下点击勾选启用用户级数据访问的凭据传递功能，这样我们才能把databricks连接到data lake
创建一个compute
同时我们点击工作区，点击workspace-shared-点create notebook。编程语言可以自选，可以选择哪个cluster
我们在notebook中试着访问datalake，这里先按照老师教的用旧方法挂载，
我们在官方文档‘使用 Microsoft Entra ID 凭据直通访问 Azure Data Lake Storage’这篇下面找到
-使用凭据直通将 Azure Data Lake Storage 装载到 DBFS-  在这里复制python代码并修改其中的我们个人创建的部分：
    # 需要改那些信息？
source = "abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/", 是数据湖的路径
<container-name> 我们创建的是bronze，<storage-account-name>我们创建的是datalake4project
因此要改成：source = "abfss://bronze@datalake4project.dfs.core.windows.net/"
mount_point = "/mnt/<mount-name>", 这是要指定访问bronze里的数据：
因此要改成mount_point = "/mnt/bronze", 

下一步建议
既然你没启用 Unity Catalog：
✅ 你可以用 dbutils.fs.mount() 来连接 ADLS Gen2 或 Blob Storage
✅ 支持使用 dbutils.fs.ls("/mnt/xxx") 来访问
✅ 可以继续使用以 /mnt/... 开头的 DBFS 路径在 Spark、pandas、SQL 中读取数据
    # dbutils.fs.ls("/mnt/bronze")是什么意思？
        在 Databricks 文件系统（DBFS）中查看 /mnt/bronze 这个挂载目录下的内容，相当于你在操作系统中执行 ls（list directory）命令。
        | 部分              | 含义                                       |
        | --------------- | ---------------------------------------- |
        | `dbutils`       | 专属的工具类库（Databricks Utilities），用于在 Notebook 中执行各种 Databricks 特有的操作                       
        | `fs`            | 文件系统（FileSystem）模块                       |
        | `ls()`          | 列出目录内容（list）                             |
        | `"/mnt/bronze"` | 挂载目录路径，通常是你通过 `dbutils.fs.mount()` 挂载的存储 |

    # 为什么要写 /mnt 开头？
        因为 /mnt 是 Databricks 文件系统（DBFS）中专门用于“挂载外部存储”的目录。
        你通过 dbutils.fs.mount() 挂载 Azure Data Lake / Blob Storage 时，必须指定一个本地路径作为挂载点（mount point），
        这个路径通常习惯写在 /mnt 下,我们再加上schema名就可以得到全部表所在的不同文件夹：

运行dbutils.fs.ls("/mnt/bronze/SalesLT")
 
接下来我们复制原来的python代码，改一下原来代码里的bronze变为silver和golden建立银层和金层即可
现在我们完成了data transform的第一部分，安装数据湖。
我们可以先读取bronze的数据进行数据转化后加入到silver层，这是第一次转化。
然后我们读取silver里的数据进行进一步转化，最后转入gold层

数据转化第二课
我们打开azure data studio，点开address 这张表，可以看到已经是比较干净的结构化的数据，我们可以学习如何使用databricks
对修改日期这一列的数据做日期格式的转化，从日期时间格式修改为日期格式。还有其他的表也需要把日期时间格式修改为日期格式。
我们学怎么在databricks中使用pyspark。
首先我们先再创建两个笔记本用来执行bronze to silver & silver to gold的操作。

bronze to silver 笔记本执行的操作：
第一步是获取数据湖中bronze中的数据:
dbutils.fs.ls('/mnt/bronze/SalesLT/')
在这一层中我们要将转化好的数据存入silver层，因此我们也调用silver层
dbutils.fs.ls('/mnt/silver/')
接下来我们定义一个路径变量 input_path,指定我们要进行数据清洗的文件的路径
input_path = '/mnt/bronze/SalesLT/Address/Address.Parquet/'
现在使用 Spark 读取 Azure 数据湖中存储的 Parquet 格式数据文件，并加载为 DataFrame。从 input_path 路径中加载数据
df = spark.read.format('parquet').load(input_path) # 将结果保存在变量 df 中，类型是 Spark 的 DataFrame。
    
    # 读取后你可以干什么？：
        df.show()         # 显示前 20 行
        df.printSchema()  # 显示字段和数据类型
        df.count()        # 看有多少行
        或者进行清洗处理、转换、写入 silver 层等操作。
    # 这里的 spark 是什么意思？spark 是干嘛的？
        Apache Spark 是一个用于处理大规模数据的分布式计算引擎，特点是：
        1能处理海量数据（比如几亿行表）
        2速度比 Hadoop MapReduce 快 10~100 倍
        3支持多种语言：Python、Scala、Java、SQL
        4支持批处理、流处理、机器学习、图计算

        类比生活中事物」来形容 Spark，可以说：
        spark 就像一个厨师大脑，你告诉它食材在哪（比如 Parquet 文件），怎么处理（比如选前10名、按地区分组），
        它就能自动调用很多厨房小助手（执行器）并行完成任务。

        #在 Databricks 中，spark 是你用来读取、分析、处理大数据的核心对象，代表了一个正在运行的 Spark 引擎。
        它能处理海量数据，并支持丰富的数据操作功能，是数据工程、数据科学的关键工具。
        #在这里spark 是一个指向 SparkSession 的变量，代表一个正在运行的 Apache Spark 引擎实例。
        在 Databricks 中，spark 是自动创建的，表示你可以直接用它来：
        读取数据（CSV、Parquet、JSON、数据库等）、处理大规模数据（清洗、转换、聚合等）、执行 SQL 查询、写出数据（到文件或数据库）

接下来我们display(df)看看。display(df) 是 Databricks Notebook 中专用的函数，用于以表格形式直观地展示 Spark DataFrame 的内容。
    # 什么是display函数：
        display(df) 是 Databricks 独有的最常用的函数之一，用来以表格和图表的形式直观查看 Spark DataFrame 的数据内容，
        比 df.show() 更适合做可视化分析和交互操作。display() 不是标准 Python 或 PySpark 的函数，只能在 Databricks Notebook 中使用。
        如果你在 Jupyter Notebook 运行，会报错。
        使用 display(df)：输出是交互式表格：可点击、可分页、可生成图表

我们要用格式化后的日期字符串（如 '2023-06-01'）直接替换掉原来的 modifiedData 列
把原本的 modifiedData 列 直接覆盖为一个新值，这个新值是：
（先把 modifiedData 从字符串转换成时间戳，再将它当作 UTC 时间处理，最后格式化成 yyyy-MM-dd 形式的字符串（只保留日期）。再把原本的 modifiedData 列直接覆盖为一个新列）
这个非常适合在数据清洗阶段做字段简化或标准化。

因此我们从 PySpark 的函数模块中，引入两个常用的日期时间函数：from_utc_timestamp 和 date_format，用于处理 Spark DataFrame 中的时间字段。
from_utc_timestamp(col, timezone) 用于把 UTC 时间转换为指定时区的本地时间 from_utc_timestamp(df["timestamp"], "Asia/Shanghai")
date_format(col, format_str)可以把时间列格式化成字符串，指定你要的格式 date_format(df["timestamp"], "yyyy-MM-dd")

from pyspark.sql.functions import from_utc_timestamp,date_format
from pyspark.sql.types import TimestampType
df = df.withColumn(
    "modifiedData",
    date_format(from_utc_timestamp(df["modifiedData"].cast(TimestampType()), "UTC"), "yyyy-MM-dd")
)
    # 为什么是 pyspark.sql？
        PySpark 是 Spark 的 Python API，整个包名是 pyspark，
        pyspark.sql 是 PySpark 中专门用于结构化数据处理的模块，里面的 functions 提供了丰富的 DataFrame 操作函数，types 提供了列的数据类型定义。
        你要处理时间格式、列类型，就必须从 pyspark.sql 引入这些模块。
        Spark 中最核心的概念是 DataFrame，而 DataFrame 相关的所有操作、函数、类型，都被统一组织在 pyspark.sql 这个模块下。
        第一行：from pyspark.sql.functions import from_utc_timestamp,date_format 是导入 SQL 模块中的函数（如时间格式转换）；
        第二行：from pyspark.sql.types import TimestampType 是导入 SQL 模块中的数据类型（你要把字符串转换成 TimestampType()，才能做时间函数处理）；
        这些函数和类型 只能 用在 DataFrame 处理上，是 Spark SQL（结构化处理）的一部分。
        模块结构图理解：
        pyspark
        │
        ├── sql              ← 专门处理结构化数据（DataFrame、SQL）的模块
        │   ├── functions.py ← 各种 DataFrame 专用函数（如 from_utc_timestamp, date_format）
        │   └── types.py     ← 各种数据类型定义（如 TimestampType, StringType 等）
        │
        ├── ml               ← 机器学习模块
        ├── streaming        ← 实时流处理模块
        └── rdd              ← 原始弹性分布式数据集模块（RDD）


    # 这些函数都是什么意思呢？
        1 最外层withColumn("modifiedData", ...)
        把整个表达式结果添加到原 DataFrame 中，覆盖原来的列名为 "modifiedData"
        2 date_format(..., 'yyyy-MM-dd')   date_format语法是date_format(col, format_str)
        可以把这个 timestamp 格式化为字符串，只保留“年月日”：
        输入：2023-06-01 15:30:00
        输出：'2023-06-01'
        3 from_utc_timestamp(..., 'UTC') 
        把转换后的 timestamp 解释为 UTC 时间，并返回对应的带时区时间（或本地时间）
            ## 什么是 UTC 时间？为什么要转换成 UTC 时间？
                UTC（Coordinated Universal Time）：是“世界标准时间”，所有服务器和数据系统处理时间时，通常都用 UTC 作为统一基准。
                它是全球通用的时间标准，不受任何地区、夏令时、国家法定时区影响。
                为什么数据存储要用 UTC 时间？大数据系统（尤其是云平台）为了保证跨时区、跨地区的数据统一性，常常把时间字段都存成 UTC 格式
                | 作用   | 原因            |
                | ---- | ------------- |
                | 全球统一 | 不受本地时间影响      |
                | 易于对比 | 跨地域计算、同步一致    |
                | 安全准确 | 避免夏令时、时区变动等问题 |
                同时Spark 不知道你给它的 timestamp 是在哪个时区存的。你得手动告诉它：“这个时间字段是 UTC 的”，它才知道怎么正确转换成你本地时间。
        
        4 df["modifiedData"].cast(TimestampType()
        将该列从字符串强制转换为 timestamp 类型，变成真正的“日期+时间”对象。
            ## 为什么要用 .cast(TimestampType())？
                因为：你从 CSV / JSON / Parquet / 数据库中读出来的时间字段，常常是字符串（StringType）
                Spark 不会自动识别这些字符串为时间，除非你手动转换。
                TimestampType() 是 Spark 中的一种数据类型，用来表示“时间戳”格式的数据，即包含“年月日 + 时分秒”的时间信息。
                所以你需要使用 .cast(TimestampType()) 把它转为 timestamp 类型，才能做时间运算、比较、格式化等操作
        
接下来我们要对其他表都进行这个操作
在databricks的notebook中，我们可以用不同的编程语言，通过一个magic command就行 
在一个单元格写% SQL，单元格的编程语言就会变为sql 我们就可以用sql了
在单元格里写%md 就会自动变为注释的markdown语言，我们可以写笔记

接下来我们使用 Python 代码读取datalake brozen路径下的文件（或目录）列表，并提取文件（或目录）的名称，用来生成一个表名列表。
table_name = [] # # 初始化一个空列表，准备用来存放表名
for i in dbutils.fs.ls('/mnt/bronze/SalesLT') # 遍历路径/mnt/bronze/SalesLT 下的所有文件或目录
    table_name.append(i.name.split('/')[0]) # 将目录名去掉/，只取名字部分添加到列表中
    # i是什么？
        dbutils.fs.ls() 会返回一个包含多个 FileInfo 对象 的列表，每个 i 就是其中的一个 FileInfo 对象，表示路径下的一个文件或目录。
        该目录下有这些文件：
        mnt/bronze/SalesLT/Customer/Customer.parquet

        dbutils.fs.ls('mnt/bronze/SalesLT/') 返回的是这样的列表,每个 i 就是其中的一个 FileInfo 
        [
          FileInfo(path='mnt/bronze/SalesLT/Customer/', name='Customer/'),
          FileInfo(path='mnt/bronze/SalesLT/Product/', name='Product/')
        ]
    # 这时 i.name 是什么? 
        第一次循环时：i.name = 'Customer/'  第二次循环时：i.name = 'Product/'
        所以i.name.split('/')[0]，当 i.name 是目录名（结尾带 /）时，它能提取目录主名。
        'Customer/'.split('/') -> ['Customer', '']
        ['Customer', ''][0] -> 'Customer'

现在我们已经拿到了所有表名的列表table_name 。我们可以对所有表进行转化
代码：

from pyspark.sql.functions import from_utc_timestamp,date_format
from pyspark.sql.types import TimestampType
for i in table_name:
    path = '/mnt/bronze/SalesLT/' + i + '/' + i + '.Parquet' 
    df = spark.read.format('parquet').load(path)
    column = df.columns # 表示 DataFrame df 的所有列名，以 Python 列表（list） 的形式返回。可以用 df.columns 来动态获取所有列名
    for col in column:
        if 'Date' in col or 'date' in col:
            df = df.withColumn(col,date_format(from_utc_timestamp(df[col].cast(TimestampType()),'UTC'),'yyyy-MM-dd')) # 为什么这的col不加'',这是因为你要传的是变量名，而不是字符串常量。
    output_path = '/mnt/silver/SaleLT/' + i + '/'
    df.write.format('delta').mode('overwrite').save(output_path) 

    # if 'Date' in col or 'date' in col: 这里不需要用正则表达式就可以判定列名里是否含'Date' or 'date'  了吗？
        'Date' in col or 'date' in col这个判断的意思是：只要列名里包含 'Date'（区分大小写）或 'date'，就返回 True。
        什么时候需要正则表达式？
        如果你想判断得更灵活或更复杂，那你可以用 re 模块（正则表达式）：
        比如：
        不区分大小写地判断是否包含 'date'
        匹配字段名结尾是 '_date' 或 'Date$'
        只匹配整个单词，而不是像 updated, created 这种

    # df.write.format('delta').mode('overwrite').save(output_path)的含义？
        把 DataFrame df 按照 Delta 格式 存储到 output_path 路径下；如果路径已有数据，就强制覆盖。
        把 DataFrame 写入存储系统的标准写法
        1. .write 是什么？
        df.write 是 DataFrameWriter 对象，是 DataFrame 的一种“输出接口”。
        它允许你将 DataFrame 写入多种存储系统：如 Parquet、Delta Lake、CSV、JDBC、Hive、S3 等。
        2. .format('delta') 是什么？
        这是指定写入的文件格式。'delta' 表示用 Delta Lake 的格式存储，这是一个支持ACID事务、版本控制、增量处理的格式，是 Databricks 推荐格式。
        常见的 format() 参数还有：
        | 格式          | 描述                 |
        | ----------- | ------------------ |
        | 'parquet' | 高效列式存储格式，支持 schema |
        | 'csv'     | 简单的文本格式            |
        | 'json'    | 写成 JSON 文件         |
        | 'delta'   | 支持事务的高级数据湖格式       |
        3. .mode('overwrite') 是什么？
        这是指定写入模式，常见有：
        | 模式            | 含义说明                  |
        | ------------- | --------------------- |
        | 'overwrite' | 如果目标路径已经存在，会覆盖原数据 |
        | 'append'    | 追加写入，不会删除原数据      |
        | 'error'（默认） | 如果目标已存在就报错            |
        | 'ignore'    | 如果目标已存在就跳过写入   |
        4. .save(output_path) 是什么？
        这是最后一步，指定写入路径。

# parquet的书写规范
虽然 .Parquet 没问题，建议你以后都使用小写 .parquet，因为：
小写 .parquet 是行业惯例；容易和代码风格统一；跨平台（Linux、Windows、S3、ADLS）更稳妥；避免其他人（或团队成员）误以为你拼错。


接下来我们要去进一步进行数据清洗，把silver里的表格转化后存到gold层
我们要对表格的列名进行操作，把连起来的英文名称按下划线分开:类似ColumnName -> Column_Name
也就是实现列名从 驼峰命名（CamelCase） 转换为 下划线命名（snake_case），这是是金银铜（Bronze → Silver → Gold）数据湖治理中的常规操作。
还是和上一层一样
我们先指定文件路径，然后spark读取文件

dbutils.fs.ls('/mnt/silver/SalesLT')
dbutils.fs.ls('/mnt/gold')
input_path = '/mnt/silver/SalesLT/Address/'
df = spark.read.format('delta').load(input_path)
display(df)
from pyspark.sql import SparkSession
from pyspark.sql.function import col,regexp_replace
column_names = df.columns
for old_col_name in column_names:
    new_col_name = ''.join(['_' + char if char.isupper() and (i == 0 or not old_column_name[i - 1].isupper())else char for i,char in enumerate(old_column_name)]).lstrip('_')
    df = df.withColumnRenamed(old_column_name,new_column_name)

    # 让i == 0返回'_' + char 不是和.lstrip('_')冲突了吗？
        i == 0 返回 '_' + char 确实会在开头加 _，而 .lstrip('_') 会把这个多余的开头 _ 删除掉。
        这是一种“先一律加 _，最后统一清理开头”的策略，虽然 .lstrip('_') 和 i == 0 看起来像是冲突，
        其实是为了写法简洁、逻辑通用而刻意配合设计的。
        在这里udemy老师没有写i == 0，因为这段代码中，如果你处理的是第一个字符（i == 0），
        char = 'C'           # 第一个字符
        old_col_name[i - 1] = 'r'  → 是小写
        not old_col_name[i - 1].isupper() = True
        所以这时候其实也会加上 _，结果是：_Customer 。最后统一再 .lstrip('_') 清除前导下划线。
        
        老师的写法简洁但依赖 Python 的负索引行为（-1 是最后一个字符），
        我们写的 i == 0 or ... 是更安全、更清晰、更易维护的写法。
        ✅ 即使后期移植到其他语言（不支持负索引），也更具可移植性。

# 导入的模块解释如下：
SparkSession是用于创建 Spark 应用的入口，用于读取、写入、配置等操作
col	用于引用 DataFrame 中的列名，例如 col("CustomerID")
regexp_replace	正则替换函数，可以对列中字符串进行替换
它们都和 withColumnRenamed() 是配合使用的工具函数，但本身不直接用于重命名。
我们这里没有使用 col()；
没有使用 regexp_replace()；
也没有重新创建 SparkSession；
Udemy 教程中的这些导入是为了方便后续教学使用（比如处理列内容），
但在你现在的列名重命名逻辑里，它们不是必须的。如果你没用 col() 或 regexp_replace()，这些导入可以删除。

# 接下来我们用到了一个列表推导式 + 三元表达式（if...else）的组合结构，用来对每个字符做“条件判断式变换”，
相当于在列表推导式中用了三元表达式，这是 Python 数据清洗中非常常见也非常高效的写法。
这不是“过滤型”的 if（比如 [x for x in list if x > 0]），而是条件选择型表达式：
[
'_' + char 
if char.isupper() and (i == 0 or not old_col_name[i - 1].isupper()) else char 
for i, char in enumerate(old_col_name)
]
本质上是 [表达式1 if 条件 else 表达式2 for 变量 in 可迭代对象]

每一步的含义： 
    # 1 什么是 enumerate()？ i 和char的含义是什么？
        在 Python 中，enumerate() 是一个内置函数，用来在循环中同时获得索引（位置）和元素的值。
        语法：for index, value in enumerate(iterable):
        char 是 character（字符）的缩写
        在 Python 或编程语言中：
        char → character（单个字符）
        str → string（字符串）
        int → integer（整数）
        bool → boolean（布尔值）
        dict → dictionary（字典/哈希表）
        所以你看到：for i, char in enumerate(old_col_name):就是在遍历 old_col_name 字符串的每个“字符”（character）。
    # 2 char.isupper() 是什么意思？ .isupper() 是 Python 字符串对象（str）的方法，用于判断一个字符是否是 大写英文字母。
    # 3 i == 0和not old_col_name[i - 1]是什么意思
        i 是当前字符的索引（位置），从 0 开始； ，char 是当前字符的值（比如 'C'、'u'、's' 等）；
        i == 0 的作用是：
        防止当前是第一个字符（i == 0），你去访问 old_col_name[-1]，虽然 Python 不报错（因为 -1 是最后一个字符），
        但逻辑会出错，你不是真的想访问最后一位。
        所以条件判断是这样的：'_' + char if char.isupper() and (i == 0 or not old_col_name[i - 1].isupper()) else char
        意思是：
        如果当前字母是大写，并且如果它是第一个字符（i == 0）而且它前一个字符不是大写（说明是新单词开头），那么直接在前面加下划线；
        否则（是连续大写的一部分），就不要加下划线。
    # 4 ''.join(...) 是什么意思？
        它可以用空字符串 '' 作为连接符，把一个“字符串列表”拼接成一个“完整字符串”。
        举个例子：
        chars = ['C', 'u', 's', 't', 'o', 'm', 'e', 'r']
        result = ''.join(chars)
        print(result)
        输出：Customer
        我们写的是：new_col_name = ''.join([...])
        这里的 [...] 是一个列表推导式，生成了像：['_c', 'u', 's', 't', 'o', 'm', 'e', 'r', '_i', 'd']
        然后用 ''.join(...) 把它拼成：'_customer_id'
        配合 .lstrip('_') 去掉前面的下划线，得到最终：'customer_id'
    # 5 为什么是.lstrip('_') 而不是strip()?
因为.lstrip('_') 中的 l 就是 left（左侧） 的意思
我们只想去掉字符串开头的下划线，而不是中间或结尾的下划线。
快速结论：
| 方法             | 含义               | 示例                                        |
| -------------- | ------------------ | --------------------------------------------|
| .lstrip('_') | 只移除左侧（开头）的 '_' | '_customer_id'.lstrip('_') → 'customer_id' |
| .strip('_')  | 移除左右两侧所有的'_'    | '_customer_id_'.strip('_') → 'customer_id' |
| .rstrip('_') | 只移除右侧（末尾）的 '_' | 'customer_id_'.rstrip('_') → 'customer_id' |


接下来我们要对所有表进行这样的操作，然后把处理好的这些表写进gold层
和之前broze to silver的非常类似
table_name = []
for i in dbutils.fs.ls('/mnt/silver/SalesLT/')
    table_name.append(i.name.split('/')[0])

for name in table_name:
    path = '/mnt/silver/SalesLT/' + name
    print(path)
    df = spark.read.format('delta').load(path)
    column_names = df.columns
    for old_column_name in column_names:
        new_column_name = ''.join(['_' + char if char.isupper() and (i == 0 or not old_column_name[i-1].isupper()) else char for i,char in enumerate(old_column_name)]).lstrip('_')
        df =df.withColumnRenamed(old_column_name,new_column_name)
    output_path = '/mnt/gold/SalesLT/' + name + '/'
    df.write.format('delta').mode('overwrite').save(output_path)

    # path = '/mnt/silver/SalesLT/' + name能访问到里面的delta文件吗，文件的路径应该是path = '/mnt/silver/SalesLT/Address/XXX'?
        是的，只要你用的是 Delta 格式，路径写到表目录即可，不需要写到具体的文件（如 .parquet 或 _delta_log 目录）也能成功读取。
        ✅ 所以这句是 可以成功访问 Delta 表的：
        为什么可以？
        Delta 表本质上是一个目录格式的数据表，Databricks 和 Spark 通过读取这个目录下的 _delta_log 元数据来知道：
        表包含哪些文件
        schema 是什么
        有哪些版本
        只要你指向的是表的主目录（比如 /mnt/silver/SalesLT/Address/），Spark 就能自动加载表。
    # 所以我们写出去的时候也只需要写到这里就行了
        output_path = '/mnt/gold/SalesLT/' + name + '/'
        df.write.format('delta').mode('overwrite').save(output_path)
        是 写入 Delta 表的标准方式，只要 output_path 指向的是一个表的目录路径，你就可以这样写，系统会自动在该目录下创建：
        .parquet 数据文件
        _delta_log/ 元数据目录


接下来我们的工作是要用adf搭建数据管道，去让bronze to silver和silver to gold变为job
我们现在把之前两个笔记本里演示的对一个表进行数据清洗的代码删除，只保留对所有表进行数据清洗的代码
storagemount是我们连接储存账户的笔记本，这个不用管，我们只需要在adf pipeline里运行bronze to silver和silver to gold
我们在adf中新建一个link service connection 来连接adf和databricks
我们选择现有交互式群集，可以直接用databricks我们现有的
身份验证类型 我们选择access token 我们去databricks页面点击name，然后点击user setting - 开发者 -访问令牌 -创建新令牌
然后我们创建了新的token 复制令牌 
现在我们把刚生成的token存在keyvalue vault里，然后连接服务选token，选择azure key vault连接，选对应的AKV 链接服务和机密名称即可
然后选择点击 ‘从现有群集中选择’ 选择我们此前在databricks中创建的集群datatransformation
选好以后我们点击测试连接，连接成功我们点击创建。
现在我们的adf可以访问databricks里的所有笔记本。我们点击全部发布
一、Databricks Linked Service 的两种常用 Cluster 类型
1. 现有交互式群集（Existing Interactive Cluster）
✅ 优点：
群集已启动，作业执行快（启动时间几乎为0）；适合调试、开发环境；在 Udemy 课程中讲的是项目演示，老师选这个是因为方便演示；
⚠️ 缺点：
手动管理群集（需提前启动），不适合自动化生产；群集空闲可能会被 Azure 停止。
2. 作业群集（Job Cluster）
✅ 优点：
每次运行 Pipeline 时，ADF 自动临时创建一个群集，任务结束自动删除；更适合 生产自动化任务（自动清理资源、节省成本）；
⚠️ 缺点：
每次执行前需要等待群集启动（一般30秒~2分钟）；设置稍复杂，要提供完整的 Databricks Runtime 配置。
二、身份验证方式（Authentication Type）区别
Access token	手动生成的访问令牌（personal access token, PAT）	✅ 开发、测试	较安全（时间限制）	从 Databricks UI 中手动生成
Azure Managed Identity（MSI）	使用 ADF 的系统托管身份或用户托管身份进行认证	✅ 推荐用于生产	更安全（不暴露密码或token）	需要在 Azure 中配置权限


    # 获取机密名称失败的潜在原因：
        注意在我们创建azure key vault的时候，机密权限（Secret permissions）：应该确保勾选了至少 Get 和 List；，
        ✅ 勾选Get → 允许读取指定名称的机密值
        但是没有勾选：❌ List 就不能列出所有机密名称
        只勾选了 Get，没有 List，导致 ADF 知道 key 的名字但无法访问它的值或列出机密，最终报错 Forbidden。
        现在只要把 List 加上，重新保存访问策略，再测试连接，💡 一切就通了。
涉及到敏感的值、密码等都应该通过这种方式去建立link service访问key vault去进行连接


接下来我们要在adf中创建两个notebook活动，一个是bronze to silver 一个是silver to gold notebook类型选择databricks
创建好了后把foreach活动成功后连接上bronze to silver 然后我们点击笔记本下的azure databircks Databricks 链接服务*选择刚才创建的连接
setting 的笔记本路径选择databricks里我们两个笔记本的路径
bronze to silver成功后连接上silver to gold

然后我们可以点击发布全部更改保存我们的pipeline
然后可以触发。由于我们之前在databricks 的notebook里写的代码是df.write.format('delta').mode('overwrite').save(output_path)
mode('overwrite')是覆写，因此我们可以直接运行，数据会把原来旧的覆盖掉。
我们在管道运行的时候，可以检查进度，点击活动旁边的glass眼镜模样的标志。
我们可以去data lake检查，可以发现delta文件增加了一个一模一样的。这是delta的功能。可以追踪更改历史记录。因此delta是被推荐存储在azure data lake上的推荐数据存储格式

三、Data loading
Azure Synapse 自动化建视图流程（Gold层 → Serverless SQL Views）
🎯 目标：
从 Azure Data Lake 的 Gold 层（/gold/SalesLT/） 获取所有子目录（每个代表一个 Delta 表），
为每个表自动在 Serverless SQL DB 中创建视图，实现全自动“读取型数据集市”连接。


接下来我们可以把处理好的gold层数据存入Azure Synapse Analytics数据库
对于我们现在结构清晰数据量小的项目，我们选择建立无服务器的sql database即可
我们的数据已经储存在数据湖中，我们可以使用内置sql池直接从数据湖中查数据
我们点data 下面的link可以看到数据湖，其中有我们创建的金银铜三层数据，点击gold-SalesLT可以看见清洗好的表格
如果只想查询某个表，可以右键点击表格，我们可以看到有一些选项可以执行sql脚本 比如查询前100行，会询问我们文件类型，我们选择delta
它自动连接到内置数据库
这样它就自动生成了sql代码：
-- This is auto-generated code
SELECT
    TOP 100 *
FROM
    OPENROWSET(
        BULK 'https://datalake4project.dfs.core.windows.net/gold/SalesLT/Address/',
        FORMAT = 'DELTA'
    ) AS [result]

运行就可以得到查询的数据
在数据湖中存储数据比sql专用数据库便宜很多
我们把刚生成的代码改成：

CREATE VIEW address
AS
SELECT
    *
FROM
    OPENROWSET(
        BULK 'https://datalake4project.dfs.core.windows.net/gold/SalesLT/Address/',
        FORMAT = 'DELTA'
    ) AS [result]

我们创建一个视图，视图（View）：是一种虚拟表，本质上是保存了一个查询结果的定义，用户可以像查询表一样查询它。
    # 各部分含义
        1 OPENROWSET(...)
        是 Synapse 中用来读取外部数据源（例如 ADLS）的函数。
        2 BULK 'https://.../SalesLT/Address/'
        表示从 Azure Data Lake Storage (ADLS Gen2) 读取路径为 gold/SalesLT/Address/ 下的数据。
        这个路径是一个目录，其中存放的是 Delta Lake 格式 的文件（例如 .parquet, _delta_log 等）。
        3 FORMAT = 'DELTA'
        告诉 Synapse 这个目录下的数据使用的是 Delta Lake 格式，这是一种支持事务的表格式，常用于大数据处理（通常和 Databricks 一起用）。
        4 AS [result]
        为 OPENROWSET 返回的数据起个别名叫 result，后续查询中可以使用它（这里直接 SELECT *，没用到别名）。
        创建一个名为 address 的视图，它的内容来自于 Azure Data Lake 中路径为 gold/SalesLT/Address/ 的一个 Delta 格式数据集。
        使用这个视图后，你就可以直接查询 SELECT * FROM address，而不必每次都写 OPENROWSET(...)。
    # 开头的AS是干什么用的：
        AS 的含义是什么？简单说就是“请帮我创建一个叫 address 的虚拟表，它的内容等于下面这条 SELECT 查询的结果。”
        在 CREATE VIEW ... AS 中，AS 的意思是：“定义这个视图的内容是下面这条 SELECT 查询的结果。”
        换句话说：
        CREATE VIEW address 是在说：“我要创建一个叫 address 的视图”
        AS 起到了“赋值”的作用
        SELECT ... 是你要定义的视图的查询内容

接下来我们在右上角要把连接的数据库从master改成gold_db 运行这段查询，显示成功后去workspace找到下面的sql database - views - dbo.address
可以看见我们刚创建的视图
    # dbo是什么？
        dbo.address 中的 dbo.，其实是 SQL Server 和 Synapse 中的一个默认架构（schema）名称
        dbo 是 "database owner" 的缩写，是 SQL Server 系统中默认的数据库架构（schema）名称。
        在 SQL Server 或 Azure Synapse 中，一个数据库中的对象（表、视图、存储过程等）都有属于自己的 schema（架构），
        而 dbo 就是其中最常见、默认使用的那个。
然后我们选择查询dbo.address的前100行，执行
如果数据湖中的表格被修改加入新列之类的，这个视图也会随之改变

现在我们不满足是给address表建立视图，我们要用pipeline动态的为所有表建立视图
我们知道Azure synapse analytic是建立在adf之上，所以我们无需在adf里建立pipeline而是直接在azure synapse analytic里建
在develop中我们可以写sql脚本用来动态储存gold中的表的视图：
代码：

USE gold_db # 使用数据库 gold_db（即在这个数据库下执行下面的操作）
GO # 表示执行上面那条语句（在 SSMS / Synapse 中用于分段执行）
CREATE OR ALTER PROC CreateSQLServerLessView_gold @ViewName NVARCHAR(100) 
# 创建或修改一个名为 CreateSQLServerLessView_gold 的存储过程 参数：@ViewName 是你传入的视图名，比如 'Address' 
AS 
BEGIN # 存储过程体开始
    DECLARE @statement NVARCHAR(MAX);
# 声明一个变量 @statement，类型是 nvarchar(max)，用于存储动态 SQL 语句
    SET @statement = 
        N'CREATE OR ALTER VIEW ' + QUOTENAME(@ViewName) + N' AS
        SELECT * 
        FROM OPENROWSET(
            BULK ''https://datalake4project.dfs.core.windows.net/gold/SalesLT/' + @ViewName + '/'',
            FORMAT = 'DELTA'
        ) AS [result]';

    EXEC(@statement); # 执行你上面动态拼接出来的 SQL 语句
END
GO # 结束存储过程定义
进一步解释
    # 一、PROC 是什么意思？
PROC 是 PROCEDURE 的简写。在 SQL 中：是在创建一个存储过程（Stored Procedure）。
存储过程（Stored Procedure）是一段可以被反复调用的 SQL 脚本，你可以把它理解为“数据库中的函数”。
存储过程存在于数据库中，可以接收参数，可以封装逻辑（如创建视图、插入数据、更新等），可以被反复执行
在这里它就是一个用来动态创建视图的自动化工具函数。
# 二、@ViewName NVARCHAR(100) 是什么意思？
这是这个存储过程的一个输入参数定义，我们逐部分讲：
CreateSQLServerLessView_gold 是过程名
@ViewName	这是参数的名字，代表你传进去的“视图名称”
NVARCHAR(100)	表示这个参数的数据类型是 Unicode 字符串，最大长度是 100 个字符
就像函数要接收参数一样，存储过程也可以接收参数，@ViewName 就是你定义的这个“变量名”。


# 三、为什么是 NVARCHAR 而不是 VARCHAR？
在 Azure Synapse 和现代 SQL Server 中，推荐用 NVARCHAR 以确保兼容各种语言字符。

| 类型              | 全称                                    | 是否支持 Unicode（多语言）    | 字符占用空间     | 场景                               |
| --------------- | ------------------------------------- | -------------------- | ---------- | -------------------------------- |
| VARCHAR(MAX)  | Variable-length ASCII character       | ❌ 只支持 ASCII 字符       | 每个字符占 1 字节 | 英文、数字、ASCII 符号为主的场景              |
| NVARCHAR(MAX) | Variable-length Unicode character | ✅ 支持中文、日文、emoji、特殊符号 | 每个字符占 2 字节 | 推荐用于动态 SQL、多语言环境、云平台（如 Synapse）中 |
max是什么
| 类型              | 最大长度               | 实际用途                          |
| --------------- | ------------------ | ----------------------------- |
| NVARCHAR(100) | 最多 100 个字符         | 常用于列名、名字、代码等                  |
| NVARCHAR(MAX) | 最多 2^31-1 个字符（2GB） | 用于存储大文本、大 SQL 语句、大 JSON、大 XML |

# 四、什么是 @statement？
在 SQL 中，我们需要先用一个变量来“装下”要执行的 SQL 代码文本，这个变量名就是 @statement。
在代码中，@statement 通常会被 EXEC(@statement) 运行。
DECLARE @statement NVARCHAR(MAX) 这句声明了一个变量 @statement，类型是 NVARCHAR(MAX)，用来存储一段 SQL 语句（也就是字符串）。

# 五、什么是 SET @statement = N'...'？ 为什么加个 N？
这是为了给变量 @statement 赋值，值就是一段用字符串写出的 SQL 语句。
N 是 National character 的意思，是 T-SQL 中Unicode 字符串的前缀。
表示这是一个 Unicode 字符串，可以支持中文、日文、韩文、emoji 等多语言字符。
变量类型是 NVARCHAR，赋值时必须用 N'...'，否则 SQL Server 会尝试做类型转换，有时候可能导致失败或字符丢失。

# 六、QUOTENAME(@ViewName) 的作用是什么？
N'create or alter view' 只是简单拼接 + @ViewName + —— 这是可以执行的，但如果你传入的视图名中有 特殊字符、空格、保留字或非标准格式，就可能报错！
QUOTENAME() 是 SQL Server 提供的一个函数，用来：给对象名（表名、字段名、视图名等）自动加上方括号 []，避免语法冲突或 SQL 注入风险。

# 七、为什么要写成as [result] 而不是as result？
别名是保留字（如 order, select, result）	✅ 必须加中括号或引号
在 SQL Server 和 Synapse 中，如果别名是 保留字、可能与关键字冲突、或包含特殊字符/大小写，就必须用中括号 [] 括起来。

# 八、FORMAT = ''DELTA''	✅	必须双写单引号，表示 SQL 字符串内的文本

我们执行并发布更改
点击链接服务，新建azure sql database 
账户选择方法我们选择Enter manually
然后去synapse-workspace-for-data-engineer-project 设置找到属性下面有无服务器 SQL 终结点，复制并粘贴到完全限定的域名
数据库名称 gold_db
身份验证类型System-assigned managed identity 尝试连接创建发布更改

接下来我们可以去创建pipeline了
拿一个获取原数据的活动,改个名字，然后去设置里选择新建一个数据湖，新建一个二进制文件，起名字叫goldtables
    # 用 Azure Synapse Analytics 的 Data Factory / Pipeline 工具 处理 Data Lake 数据，
    并用 Get Metadata 获取文件信息，为什么新建数据集要选“二进制（Binary）” 类型？
        遇到的问题场景总结：
        Data Lake 的 Gold 层有很多 Delta 表（路径）
        想通过 Synapse Pipeline 把它们“转成” SQL Serverless 视图（在 SQL DB 中查看）
        想用 Get Metadata 自动获取这些表的路径或子文件夹名，动态建视图
        Get Metadata 新建数据集的时候提示必须选 Binary 数据集

        因为 Get Metadata 并不读取文件内容，而是读取“文件本身的元数据”（如文件名、大小、修改时间等），
        而这不依赖文件的格式（Parquet/CSV/Delta），所以必须选 Binary。

       

直接选择数据湖的连接synapse-workspace-for-data-engineer-project-WorkspaceDefaultStorage
路径选gold-SalesLT 新建
然后我们在设置这里字段列表下新建选一个child item的参数，这样可以获得所有 SalesLT下的子项 调试一下可以成功
然后我们再添加一个foreach活动 设置里我们选原数据动态获得内容然后点子项@activity('Get tablenames').output.childItems
这样可以将整个数组传递给每个循环。然后我们点编辑foreach活动，添加Stored procedure活动
设置里选连接serverlessSQLdb，存储过程名称选之前创建的dbo].[CreateSQLServerLessView_gold]
存储过程参数 新建一个名字是我们sql代码中的ViewName 类型string 值因为我们是for each活动所以选择动态添加 @item().name

现在我们可以发布并运行这个管道

# 流程步骤汇总
🔹1. 新建一个 Synapse Pipeline
🔹2. 添加 Get Metadata 活动（命名为：Get tablenames）
连接服务：选择你默认的数据湖存储（如 synapse-workspace-for-data-engineer-project-WorkspaceDefaultStorage）
路径：设置为 gold/SalesLT（这是你的 Gold 层）
字段列表：点击添加 → 选择 child items
✅ 这个设置的作用是：获取 SalesLT 文件夹下的所有子目录名（即所有 Delta 表名）
🔹3. 添加 ForEach 活动
名称：ForEach - create views
Items 设置：点击右上角的小动态表达式图标，输入：
@activity('Get tablenames').output.childItems
✅ 这句表示获取 Get tablenames 输出的所有子项（如 Address、Customer、Product 等）
🔹4. 编辑 ForEach 内部：添加 Stored Procedure 活动
名称：Create View
Linked service：选择你连接的 Serverless SQL DB（比如 serverlessSQLdb）
Stored procedure name：选择你已创建的过程：[dbo].[CreateSQLServerLessView_gold]
这个就是前面写的 SQL 脚本创建的存储过程：ForEach 就会逐个循环每个表名（如 Address, Customer...）。
在 ForEach 内的 Stored Procedure 活动中：参数名称：ViewName 类型：String 值设为：@item().name
➡️ 表示当前循环到的 Delta 表名（子文件夹名）将被作为参数 @ViewName 传给过程。
🔹5. 设置存储过程参数
新建参数：
参数名：ViewName（必须与你过程定义中的参数名一致）
类型：String
值：点击动态表达式图标，输入：@item().name
✅ 这句表示从当前 ForEach 循环中的每一个子项，取其名称作为视图名。
✅ 6. 发布并运行
点击 发布（Publish），然后 手动触发运行（Trigger now）
每个子文件夹下的 Delta 表将自动在 SQL 数据库中创建一个视图
如SalesLT/Address/ → 创建视图 [Address]
如SalesLT/Product/ → 创建视图 [Product]
在SQL Serverless 数据库中可以看到这些视图，直接 SELECT * FROM [Address] 即可查询 Delta 表

Pipeline 会自动完成以下操作：
| 步骤 | 操作                                                 
| --- | -------------------------------------------------- 
| 1️⃣ | 遍历 gold/SalesLT/ 下所有子文件夹名                        
| 2️⃣ | 对每个子表名调用一次 CreateSQLServerLessView_gold         
| 3️⃣ | 存储过程接收这个表名，拼接 SQL 语句                               
| 4️⃣ | 动态创建视图，例如 CREATE OR ALTER VIEW [Customer] AS ... 
| ✅   | 整个过程实现“批量建视图”的自动化逻辑          

一、项目一句话介绍（英文/中文双语）
🔹 英文：
"Designed and implemented a metadata-driven data ingestion pipeline in Azure Synapse to automatically create Serverless SQL views from Delta tables stored in the Data Lake Gold layer, enabling easy querying and downstream BI integration."
🔹 中文：
在 Azure Synapse 中设计并实现了一个元数据驱动的自动建视图流程，从 Data Lake Gold 层的 Delta 表中自动生成 Serverless SQL 视图，实现数据湖的结构化建模和高效查询接入。
✅ 二、你项目的主要组成部分（结构讲法）
面试讲的时候可以用 STAR（情况-任务-行动-结果）结构来讲：
🔸 Situation（背景）：
公司/学习项目中，我需要将 Data Lake 中的 Gold 层数据暴露给数据分析师查询，同时避免他们直接处理原始路径和 Delta 文件结构。
🔸 Task（任务）：
我的目标是搭建一个自动化流程，根据 Gold 层中的 Delta 表目录，动态创建对应的 Serverless SQL 视图，做到数据湖向 SQL 层的结构化抽象，并保证未来能动态扩展和维护。
🔸 Action（我做了什么）：
使用 Azure Synapse Pipelines 中的 Get Metadata 活动，获取 /gold/SalesLT/ 下所有 Delta 表子文件夹（如 Address、Customer）。
使用 ForEach 循环逐个读取表名，并调用我自定义的 存储过程 CreateSQLServerLessView_gold，该过程通过 OPENROWSET + FORMAT='DELTA' 拼接动态 SQL，创建视图。
每个子文件夹即对应一个 Delta 表目录，自动生成如 [Address], [Customer] 等 Serverless SQL 视图。
所有视图存储在 Serverless SQL DB 中，供分析师通过 SQL 查询使用，实现与 Power BI 和 Azure ML 的无缝对接。
🔸 Result（成果）：
减少了 100% 的手动视图创建工作量
实现了数据湖到 SQL 语义层的自动对接
提高了数据建模效率，降低了误操作和重复劳动
支持未来任意添加新表，系统会自动同步建视图




Data Reporting using Power BI

安全和治理 
创建安全的资源组和分配适当的权限对于做任何实时数据工程的项目都非常重要
之前我们用了azure key vault存储密码，现在我们尝试使用Azure Active Directory进行访问控制
我们看data-engineering-projects | 访问控制(标识和访问管理) ｜角色分配 
这里只有我自己的一个账户可以访问这个资源组，但在实时的数据工程项目中，实际上会有多个工程师同时在线上进行操作
但如果我们手动的一个个添加他们在这里费时费力，因此我们需要使用azure active directory

Azure Active Directory（Azure AD） 是微软提供的一种云端身份与访问管理服务，主要用于：
一、身份验证与用户管理
Azure AD 是企业管理用户身份的核心平台，它可以：
管理员工、合作伙伴、客户的身份信息
支持用户登录各种服务（如 Microsoft 365、Azure、Teams、SharePoint 等）
提供 单点登录（Single Sign-On, SSO），用户登录一次就可以访问多个服务
✅ 二、权限控制与安全策略
Azure AD 可以对不同用户设定不同的权限，常见功能包括：
多重身份验证（MFA）：提高账户安全
条件访问（Conditional Access）：根据用户位置、设备状态、风险等级等动态判断是否允许访问
角色分配（RBAC）：为用户赋予特定角色（如读者、管理员）
✅ 三、应用集成
支持集成成千上万种 SaaS 应用（如 Salesforce、Zoom、Dropbox）
也支持企业自建的应用（Web App、API）使用 Azure AD 认证
OAuth、OpenID Connect、SAML 等标准协议都可以支持
✅ 四、设备与资源接入
Azure AD 还可以用来管理：
登录 Windows 10/11 的设备（Azure AD Join）
通过 Intune 统一配置和管理设备策略
✅ 五、与本地 AD 集成（Hybrid）
如果企业以前使用的是 本地 Active Directory（传统AD），Azure AD 可以：
通过 Azure AD Connect 同步用户和密码
实现混合身份环境（Hybrid Identity）
📌 举个例子
假设你在一家公司工作，你每天需要：
登录 Outlook 查看邮件
登录 Power BI 查看报表
登录 Teams 开会
这些操作都可以通过 Azure AD 实现单点登录和统一权限控制，管理员也可以通过 Azure AD 查看日志、强制你启用 MFA、限制你只能在公司电脑登录。

我们把新人添加至Azure AD中，新人就可以获得所需要的访问权限
现在我们在home中点击azure active directory 创建组选择我们的账号
回到data-engineering-projects | 访问控制(标识和访问管理) ｜ 点add ｜添加contributor选择新人｜之后新人就有访问这个资源组的权限了


end to end pipeline text
一旦我们在sql server database更改数据加入row等，我们运行管道的话数据将会自动执行etl流程，最终在powerbi展示
在我们之前的学习中，不管是debug还是add trigger都是手动的只运行一次，但在真正的实时项目中都是应该自动运行的，
因此我们点击add trigger点击new edit 添加new trigger 起一个新名字scheduled_trigger
type选schedule 指定开始时间和时区，然后可以设置每一天重复一次
如果我们在sql数据库里修改了信息，那么到时数据管道自动执行会自动在整个流程上更新数据
